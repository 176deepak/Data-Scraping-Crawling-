{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f460a68d-ede9-4a39-840f-7350d45cd043",
   "metadata": {},
   "source": [
    "<center style = \"color: purple; font-weight: bold\"><h1>Scrapping - Analystt.ai Assignment</h1></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53746d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# required modules loading\n",
    "import os\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee38b4f-e686-49f4-a728-52ef2856dd42",
   "metadata": {},
   "source": [
    "## Data Scraper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba13d8f0-faf8-4e37-80d9-61f3b92822fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Data Crawling\n",
    "class DataCrawler:\n",
    "    def __init__(\n",
    "        self,  \n",
    "        header = \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.5938.152 Safari/537.36\"\n",
    "    ):\n",
    "        self.header = header                          # system specification and browser version name etc.\n",
    "        self.options = webdriver.ChromeOptions()      # here we uses chrome as browser \n",
    "        self.options.add_argument(self.header)             # adding specification \n",
    "        self.driver = webdriver.Chrome(options = self.options)     # initializing chrome driver\n",
    "        self.driver.maximize_window()              # increases the size of window or tab with maximum size\n",
    "\n",
    "        \n",
    "    # Part 1 Crawling\n",
    "    def part1_crawler(self, BASE_URL):\n",
    "        '''\n",
    "        Part-1 Crawling \n",
    "        • Product URL\n",
    "        • Product Name\n",
    "        • Product Price\n",
    "        • Rating\n",
    "        • Number of reviews\n",
    "        '''\n",
    "\n",
    "        # Creating a DataFrame to store crawled data\n",
    "        crawled_data = pd.DataFrame(columns = ['Product_URL', 'Product_Name', 'Product_Price', 'Product_Rating', 'Product_Reviews'])\n",
    "        # Opening the Base URL page\n",
    "        self.driver.get(BASE_URL)\n",
    "\n",
    "        # performing crawling over 20 product listing pages \n",
    "        for i in range(1, 21):\n",
    "            print(f\"Crawling page {i}: \", end = \"\")\n",
    "            \n",
    "            '''\n",
    "            check: if page iteration == 20! Because 20 is last page for crawling and this not be linked with next page. \n",
    "            So when i becomes 20, there can be error occured of Time limit error\n",
    "            '''\n",
    "            if i == 20:\n",
    "                pass\n",
    "            else:\n",
    "                WebDriverWait(self.driver, 50).until(EC.element_to_be_clickable(\n",
    "                    (By.XPATH, \"//a[@class='s-pagination-item s-pagination-next s-pagination-button s-pagination-separator']\")\n",
    "                ))\n",
    "\n",
    "            # finding all product which is listed on webpage\n",
    "            products = self.driver.find_elements(By.XPATH, '//div[@data-component-type=\"s-search-result\"]')\n",
    "            # iterate over all products\n",
    "            for product in products:\n",
    "                # product-page link [XPATH ---> a[@class = 'a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal'] ]\n",
    "                links = product.find_elements(\n",
    "                    By.XPATH, \".//a[@class = 'a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal']\"\n",
    "                )\n",
    "                # product name [XPATH ---> //span[@class = 'a-size-medium a-color-base a-text-normal'] ]\n",
    "                names = product.find_elements(By.XPATH, \".//span[@class = 'a-size-medium a-color-base a-text-normal']\")\n",
    "                # product price [XPATH ---> //span[@class='a-price-whole'] ]\n",
    "                prices = product.find_elements(By.XPATH, \".//span[@class='a-price-whole']\")\n",
    "                # product rating and reviews [XPATH ---> //div[@class='a-row a-size-small']/span]\n",
    "                ratings_box = product.find_elements(By.XPATH, \".//div[@class='a-row a-size-small']/span\")\n",
    "\n",
    "                # extracting the details of product\n",
    "                for link, name, price in zip(links, names, prices):\n",
    "                    product_url = link.get_attribute('href')\n",
    "                    product_name = name.text\n",
    "                    product_price = price.text\n",
    "                    '''\n",
    "                    check: if any product is no rated by customers or not reviews by customers\n",
    "                    if yes, then make their ratings and reviews 0\n",
    "                    '''\n",
    "                    if ratings_box == []:\n",
    "                        product_rating = product_review = 0\n",
    "                    else:\n",
    "                        product_rating = ratings_box[0].get_attribute('aria-label')\n",
    "                        product_review = ratings_box[1].get_attribute('aria-label')\n",
    "                    \n",
    "                    new_row = {\n",
    "                        \"Product_URL\": product_url, \n",
    "                        \"Product_Name\": product_name, \n",
    "                        \"Product_Price\": product_price, \n",
    "                        \"Product_Rating\": product_rating, \n",
    "                        \"Product_Reviews\": product_review\n",
    "                    }\n",
    "                    # inserting into dataframe\n",
    "                    crawled_data.loc[len(crawled_data)] = new_row\n",
    "            \n",
    "            print(f\"Completed\")\n",
    "\n",
    "            # check: if this is a last page, if this is a last page we break the loop\n",
    "            if i == 20:   \n",
    "                break\n",
    "            else:\n",
    "                self.driver.find_element(By.XPATH, \"//a[@class='s-pagination-item s-pagination-next s-pagination-button s-pagination-separator']\").click()\n",
    "        \n",
    "        self.driver.back()           # closing the driver or window after all pages crawling\n",
    "        crawled_data.to_csv(\"part1_crawled_data.csv\", index=False)    # saving the crawled data into hard disk\n",
    "        print(f\"crawled data saved at location {os.getcwd()}/part1_crawled_data.csv\")\n",
    "\n",
    "        return list(crawled_data['Product_URL'])\n",
    "        \n",
    "    \n",
    "    # Part 2 Crawling\n",
    "    def part2_crawler(self, product_URLs):\n",
    "        # dataframe for storing crawled data \n",
    "        crawled_data = pd.DataFrame(columns = ['Description', 'ASIN', 'Product_Description', 'Manufacturer'])\n",
    "        \n",
    "        # crawling each page\n",
    "        i = 1\n",
    "        for URL in product_URLs:\n",
    "            print(f\"Crawling {i} product: \", end = \"\")\n",
    "            # opening given URL or webpage\n",
    "            self.driver.get(URL)\n",
    "            try:\n",
    "                # crawling product descrition 1\n",
    "                description1 = self.driver.find_elements(By.XPATH, \"//div[@id = 'feature-bullets']/ul/li/span\")\n",
    "                # crawling product description 2\n",
    "                description2 = self.driver.find_elements(By.XPATH, \"//div[@id = 'productDescription']/p\")\n",
    "                # crawling details like, manufacturer, ASIN no etc.\n",
    "                product_details = self.driver.find_elements(By.XPATH, \"//div[@id = 'detailBullets_feature_div']/ul/li/span\")\n",
    "                # extracting description1 text\n",
    "                description1_text = ''\n",
    "                for i_description in description1:\n",
    "                    description1_text += i_description.text\n",
    "                # extracting description2 text\n",
    "                description2_text = ''\n",
    "                for i_description in description2:\n",
    "                    description2_text += i_description.text\n",
    "                # ASIN number\n",
    "                ASIN_num = product_details[3].text\n",
    "                # manufacturer \n",
    "                manufacturer = product_details[2].text\n",
    "                \n",
    "                new_row = {\n",
    "                    'Description': description1_text, \n",
    "                    'ASIN': ASIN_num, \n",
    "                    'Product_Description': description2_text, \n",
    "                    'Manufacturer': manufacturer\n",
    "                }\n",
    "                # inserting new row into dataframe\n",
    "                crawled_data.loc[len(crawled_data)] = new_row\n",
    "\n",
    "                # back to previous window\n",
    "                driver.back()\n",
    "            except:\n",
    "                pass\n",
    "            print(f\"Completed\")\n",
    "            i += 1\n",
    "\n",
    "        # saving crawled data into hard disk\n",
    "        crawled_data.to_csv('part2_crawled_data.csv', index=False)\n",
    "        print(f\"crawled data saved at location {os.getcwd()}/part2_crawled_data.csv\")\n",
    "        self.driver.close()       # closing the driver window\n",
    "        return \"Data crawled successfully\"\n",
    "\n",
    "    \n",
    "    def main(self, BASE_URL):\n",
    "        print(\"Part 1 crawling started......\")\n",
    "        URLs = self.part1_crawler(BASE_URL)\n",
    "        print(\"Part 1 crawling completed successfully......\")\n",
    "\n",
    "        print(\"Part 2 crawling started.....\")\n",
    "        _ = self.part2_crawler(URLs)\n",
    "        print(\"Part 2 crawling completed successfully.....\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbf0617-138f-4ceb-9f75-e0799dc776dc",
   "metadata": {},
   "source": [
    "### Base URL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8b68156",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = 'https://www.amazon.in/s?k=bags&crid=2M096C61O4MLT&qid=1653308124&sprefix=ba%2Caps%2C283&ref=sr_pg_1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d70ff1-0803-43a7-911a-b4ccd59acc42",
   "metadata": {},
   "source": [
    "### starting crawling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ff7de87-b7df-463e-8a02-8884b7a70b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 1 crawling started......\n",
      "Crawling page 1: Completed\n",
      "Crawling page 2: Completed\n",
      "Crawling page 3: Completed\n",
      "Crawling page 4: Completed\n",
      "Crawling page 5: Completed\n",
      "Crawling page 6: Completed\n",
      "Crawling page 7: Completed\n",
      "Crawling page 8: Completed\n",
      "Crawling page 9: Completed\n",
      "Crawling page 10: Completed\n",
      "Crawling page 11: Completed\n",
      "Crawling page 12: Completed\n",
      "Crawling page 13: Completed\n",
      "Crawling page 14: Completed\n",
      "Crawling page 15: Completed\n",
      "Crawling page 16: Completed\n",
      "Crawling page 17: Completed\n",
      "Crawling page 18: Completed\n",
      "Crawling page 19: Completed\n",
      "Crawling page 20: Completed\n",
      "crawled data saved at location W:\\analystt.ai/part1_crawled_data.csv\n",
      "Part 1 crawling completed successfully......\n",
      "Part 2 crawling started.....\n",
      "Crawling 1 product: Completed\n",
      "Crawling 2 product: Completed\n",
      "Crawling 3 product: Completed\n",
      "Crawling 4 product: Completed\n",
      "Crawling 5 product: Completed\n",
      "Crawling 6 product: Completed\n",
      "Crawling 7 product: Completed\n",
      "Crawling 8 product: Completed\n",
      "Crawling 9 product: Completed\n",
      "Crawling 10 product: Completed\n",
      "Crawling 11 product: Completed\n",
      "Crawling 12 product: Completed\n",
      "Crawling 13 product: Completed\n",
      "Crawling 14 product: Completed\n",
      "Crawling 15 product: Completed\n",
      "Crawling 16 product: Completed\n",
      "Crawling 17 product: Completed\n",
      "Crawling 18 product: Completed\n",
      "Crawling 19 product: Completed\n",
      "Crawling 20 product: Completed\n",
      "Crawling 21 product: Completed\n",
      "Crawling 22 product: Completed\n",
      "Crawling 23 product: Completed\n",
      "Crawling 24 product: Completed\n",
      "Crawling 25 product: Completed\n",
      "Crawling 26 product: Completed\n",
      "Crawling 27 product: Completed\n",
      "Crawling 28 product: Completed\n",
      "Crawling 29 product: Completed\n",
      "Crawling 30 product: Completed\n",
      "Crawling 31 product: Completed\n",
      "Crawling 32 product: Completed\n",
      "Crawling 33 product: Completed\n",
      "Crawling 34 product: Completed\n",
      "Crawling 35 product: Completed\n",
      "Crawling 36 product: Completed\n",
      "Crawling 37 product: Completed\n",
      "Crawling 38 product: Completed\n",
      "Crawling 39 product: Completed\n",
      "Crawling 40 product: Completed\n",
      "Crawling 41 product: Completed\n",
      "Crawling 42 product: Completed\n",
      "Crawling 43 product: Completed\n",
      "Crawling 44 product: Completed\n",
      "Crawling 45 product: Completed\n",
      "Crawling 46 product: Completed\n",
      "Crawling 47 product: Completed\n",
      "Crawling 48 product: Completed\n",
      "Crawling 49 product: Completed\n",
      "Crawling 50 product: Completed\n",
      "Crawling 51 product: Completed\n",
      "Crawling 52 product: Completed\n",
      "Crawling 53 product: Completed\n",
      "Crawling 54 product: Completed\n",
      "Crawling 55 product: Completed\n",
      "Crawling 56 product: Completed\n",
      "Crawling 57 product: Completed\n",
      "Crawling 58 product: Completed\n",
      "Crawling 59 product: Completed\n",
      "Crawling 60 product: Completed\n",
      "Crawling 61 product: Completed\n",
      "Crawling 62 product: Completed\n",
      "Crawling 63 product: Completed\n",
      "Crawling 64 product: Completed\n",
      "Crawling 65 product: Completed\n",
      "Crawling 66 product: Completed\n",
      "Crawling 67 product: Completed\n",
      "Crawling 68 product: Completed\n",
      "Crawling 69 product: Completed\n",
      "Crawling 70 product: Completed\n",
      "Crawling 71 product: Completed\n",
      "Crawling 72 product: Completed\n",
      "Crawling 73 product: Completed\n",
      "Crawling 74 product: Completed\n",
      "Crawling 75 product: Completed\n",
      "Crawling 76 product: Completed\n",
      "Crawling 77 product: Completed\n",
      "Crawling 78 product: Completed\n",
      "Crawling 79 product: Completed\n",
      "Crawling 80 product: Completed\n",
      "Crawling 81 product: Completed\n",
      "Crawling 82 product: Completed\n",
      "Crawling 83 product: Completed\n",
      "Crawling 84 product: Completed\n",
      "Crawling 85 product: Completed\n",
      "Crawling 86 product: Completed\n",
      "Crawling 87 product: Completed\n",
      "Crawling 88 product: Completed\n",
      "Crawling 89 product: Completed\n",
      "Crawling 90 product: Completed\n",
      "Crawling 91 product: Completed\n",
      "Crawling 92 product: Completed\n",
      "Crawling 93 product: Completed\n",
      "Crawling 94 product: Completed\n",
      "Crawling 95 product: Completed\n",
      "Crawling 96 product: Completed\n",
      "Crawling 97 product: Completed\n",
      "Crawling 98 product: Completed\n",
      "Crawling 99 product: Completed\n",
      "Crawling 100 product: Completed\n",
      "Crawling 101 product: Completed\n",
      "Crawling 102 product: Completed\n",
      "Crawling 103 product: Completed\n",
      "Crawling 104 product: Completed\n",
      "Crawling 105 product: Completed\n",
      "Crawling 106 product: Completed\n",
      "Crawling 107 product: Completed\n",
      "Crawling 108 product: Completed\n",
      "Crawling 109 product: Completed\n",
      "Crawling 110 product: Completed\n",
      "Crawling 111 product: Completed\n",
      "Crawling 112 product: Completed\n",
      "Crawling 113 product: Completed\n",
      "Crawling 114 product: Completed\n",
      "Crawling 115 product: Completed\n",
      "Crawling 116 product: Completed\n",
      "Crawling 117 product: Completed\n",
      "Crawling 118 product: Completed\n",
      "Crawling 119 product: Completed\n",
      "Crawling 120 product: Completed\n",
      "Crawling 121 product: Completed\n",
      "Crawling 122 product: Completed\n",
      "Crawling 123 product: Completed\n",
      "Crawling 124 product: Completed\n",
      "Crawling 125 product: Completed\n",
      "Crawling 126 product: Completed\n",
      "Crawling 127 product: Completed\n",
      "Crawling 128 product: Completed\n",
      "Crawling 129 product: Completed\n",
      "Crawling 130 product: Completed\n",
      "Crawling 131 product: Completed\n",
      "Crawling 132 product: Completed\n",
      "Crawling 133 product: Completed\n",
      "Crawling 134 product: Completed\n",
      "Crawling 135 product: Completed\n",
      "Crawling 136 product: Completed\n",
      "Crawling 137 product: Completed\n",
      "Crawling 138 product: Completed\n",
      "Crawling 139 product: Completed\n",
      "Crawling 140 product: Completed\n",
      "Crawling 141 product: Completed\n",
      "Crawling 142 product: Completed\n",
      "Crawling 143 product: Completed\n",
      "Crawling 144 product: Completed\n",
      "Crawling 145 product: Completed\n",
      "Crawling 146 product: Completed\n",
      "Crawling 147 product: Completed\n",
      "Crawling 148 product: Completed\n",
      "Crawling 149 product: Completed\n",
      "Crawling 150 product: Completed\n",
      "Crawling 151 product: Completed\n",
      "Crawling 152 product: Completed\n",
      "Crawling 153 product: Completed\n",
      "Crawling 154 product: Completed\n",
      "Crawling 155 product: Completed\n",
      "Crawling 156 product: Completed\n",
      "Crawling 157 product: Completed\n",
      "Crawling 158 product: Completed\n",
      "Crawling 159 product: Completed\n",
      "Crawling 160 product: Completed\n",
      "Crawling 161 product: Completed\n",
      "Crawling 162 product: Completed\n",
      "Crawling 163 product: Completed\n",
      "Crawling 164 product: Completed\n",
      "Crawling 165 product: Completed\n",
      "Crawling 166 product: Completed\n",
      "Crawling 167 product: Completed\n",
      "Crawling 168 product: Completed\n",
      "Crawling 169 product: Completed\n",
      "Crawling 170 product: Completed\n",
      "Crawling 171 product: Completed\n",
      "Crawling 172 product: Completed\n",
      "Crawling 173 product: Completed\n",
      "Crawling 174 product: Completed\n",
      "Crawling 175 product: Completed\n",
      "Crawling 176 product: Completed\n",
      "Crawling 177 product: Completed\n",
      "Crawling 178 product: Completed\n",
      "Crawling 179 product: Completed\n",
      "Crawling 180 product: Completed\n",
      "Crawling 181 product: Completed\n",
      "Crawling 182 product: Completed\n",
      "Crawling 183 product: Completed\n",
      "Crawling 184 product: Completed\n",
      "Crawling 185 product: Completed\n",
      "Crawling 186 product: Completed\n",
      "Crawling 187 product: Completed\n",
      "Crawling 188 product: Completed\n",
      "Crawling 189 product: Completed\n",
      "Crawling 190 product: Completed\n",
      "Crawling 191 product: Completed\n",
      "Crawling 192 product: Completed\n",
      "Crawling 193 product: Completed\n",
      "Crawling 194 product: Completed\n",
      "Crawling 195 product: Completed\n",
      "Crawling 196 product: Completed\n",
      "Crawling 197 product: Completed\n",
      "Crawling 198 product: Completed\n",
      "Crawling 199 product: Completed\n",
      "Crawling 200 product: Completed\n",
      "Crawling 201 product: Completed\n",
      "Crawling 202 product: Completed\n",
      "Crawling 203 product: Completed\n",
      "Crawling 204 product: Completed\n",
      "Crawling 205 product: Completed\n",
      "Crawling 206 product: Completed\n",
      "Crawling 207 product: Completed\n",
      "Crawling 208 product: Completed\n",
      "Crawling 209 product: Completed\n",
      "Crawling 210 product: Completed\n",
      "Crawling 211 product: Completed\n",
      "Crawling 212 product: Completed\n",
      "Crawling 213 product: Completed\n",
      "Crawling 214 product: Completed\n",
      "Crawling 215 product: Completed\n",
      "Crawling 216 product: Completed\n",
      "Crawling 217 product: Completed\n",
      "Crawling 218 product: Completed\n",
      "Crawling 219 product: Completed\n",
      "Crawling 220 product: Completed\n",
      "Crawling 221 product: Completed\n",
      "Crawling 222 product: Completed\n",
      "Crawling 223 product: Completed\n",
      "Crawling 224 product: Completed\n",
      "Crawling 225 product: Completed\n",
      "Crawling 226 product: Completed\n",
      "Crawling 227 product: Completed\n",
      "Crawling 228 product: Completed\n",
      "Crawling 229 product: Completed\n",
      "Crawling 230 product: Completed\n",
      "Crawling 231 product: Completed\n",
      "Crawling 232 product: Completed\n",
      "Crawling 233 product: Completed\n",
      "Crawling 234 product: Completed\n",
      "Crawling 235 product: Completed\n",
      "Crawling 236 product: Completed\n",
      "Crawling 237 product: Completed\n",
      "Crawling 238 product: Completed\n",
      "Crawling 239 product: Completed\n",
      "Crawling 240 product: Completed\n",
      "Crawling 241 product: Completed\n",
      "Crawling 242 product: Completed\n",
      "Crawling 243 product: Completed\n",
      "Crawling 244 product: Completed\n",
      "Crawling 245 product: Completed\n",
      "Crawling 246 product: Completed\n",
      "Crawling 247 product: Completed\n",
      "Crawling 248 product: Completed\n",
      "Crawling 249 product: Completed\n",
      "Crawling 250 product: Completed\n",
      "Crawling 251 product: Completed\n",
      "Crawling 252 product: Completed\n",
      "Crawling 253 product: Completed\n",
      "Crawling 254 product: Completed\n",
      "Crawling 255 product: Completed\n",
      "Crawling 256 product: Completed\n",
      "Crawling 257 product: Completed\n",
      "Crawling 258 product: Completed\n",
      "Crawling 259 product: Completed\n",
      "Crawling 260 product: Completed\n",
      "Crawling 261 product: Completed\n",
      "Crawling 262 product: Completed\n",
      "Crawling 263 product: Completed\n",
      "Crawling 264 product: Completed\n",
      "Crawling 265 product: Completed\n",
      "Crawling 266 product: Completed\n",
      "Crawling 267 product: Completed\n",
      "Crawling 268 product: Completed\n",
      "Crawling 269 product: Completed\n",
      "Crawling 270 product: Completed\n",
      "Crawling 271 product: Completed\n",
      "Crawling 272 product: Completed\n",
      "Crawling 273 product: Completed\n",
      "Crawling 274 product: Completed\n",
      "Crawling 275 product: Completed\n",
      "Crawling 276 product: Completed\n",
      "Crawling 277 product: Completed\n",
      "Crawling 278 product: Completed\n",
      "Crawling 279 product: Completed\n",
      "Crawling 280 product: Completed\n",
      "Crawling 281 product: Completed\n",
      "Crawling 282 product: Completed\n",
      "Crawling 283 product: Completed\n",
      "Crawling 284 product: Completed\n",
      "Crawling 285 product: Completed\n",
      "Crawling 286 product: Completed\n",
      "Crawling 287 product: Completed\n",
      "Crawling 288 product: Completed\n",
      "Crawling 289 product: Completed\n",
      "Crawling 290 product: Completed\n",
      "Crawling 291 product: Completed\n",
      "Crawling 292 product: Completed\n",
      "Crawling 293 product: Completed\n",
      "Crawling 294 product: Completed\n",
      "Crawling 295 product: Completed\n",
      "Crawling 296 product: Completed\n",
      "Crawling 297 product: Completed\n",
      "Crawling 298 product: Completed\n",
      "Crawling 299 product: Completed\n",
      "Crawling 300 product: Completed\n",
      "Crawling 301 product: Completed\n",
      "Crawling 302 product: Completed\n",
      "Crawling 303 product: Completed\n",
      "Crawling 304 product: Completed\n",
      "Crawling 305 product: Completed\n",
      "Crawling 306 product: Completed\n",
      "Crawling 307 product: Completed\n",
      "Crawling 308 product: Completed\n",
      "Crawling 309 product: Completed\n",
      "Crawling 310 product: Completed\n",
      "Crawling 311 product: Completed\n",
      "Crawling 312 product: Completed\n",
      "Crawling 313 product: Completed\n",
      "Crawling 314 product: Completed\n",
      "Crawling 315 product: Completed\n",
      "Crawling 316 product: Completed\n",
      "Crawling 317 product: Completed\n",
      "Crawling 318 product: Completed\n",
      "Crawling 319 product: Completed\n",
      "Crawling 320 product: Completed\n",
      "Crawling 321 product: Completed\n",
      "Crawling 322 product: Completed\n",
      "Crawling 323 product: Completed\n",
      "Crawling 324 product: Completed\n",
      "Crawling 325 product: Completed\n",
      "Crawling 326 product: Completed\n",
      "Crawling 327 product: Completed\n",
      "Crawling 328 product: Completed\n",
      "Crawling 329 product: Completed\n",
      "Crawling 330 product: Completed\n",
      "Crawling 331 product: Completed\n",
      "Crawling 332 product: Completed\n",
      "Crawling 333 product: Completed\n",
      "Crawling 334 product: Completed\n",
      "Crawling 335 product: Completed\n",
      "Crawling 336 product: Completed\n",
      "Crawling 337 product: Completed\n",
      "Crawling 338 product: Completed\n",
      "Crawling 339 product: Completed\n",
      "Crawling 340 product: Completed\n",
      "Crawling 341 product: Completed\n",
      "Crawling 342 product: Completed\n",
      "Crawling 343 product: Completed\n",
      "Crawling 344 product: Completed\n",
      "Crawling 345 product: Completed\n",
      "Crawling 346 product: Completed\n",
      "Crawling 347 product: Completed\n",
      "Crawling 348 product: Completed\n",
      "Crawling 349 product: Completed\n",
      "Crawling 350 product: Completed\n",
      "Crawling 351 product: Completed\n",
      "Crawling 352 product: Completed\n",
      "Crawling 353 product: Completed\n",
      "Crawling 354 product: Completed\n",
      "Crawling 355 product: Completed\n",
      "Crawling 356 product: Completed\n",
      "Crawling 357 product: Completed\n",
      "Crawling 358 product: Completed\n",
      "Crawling 359 product: Completed\n",
      "Crawling 360 product: Completed\n",
      "Crawling 361 product: Completed\n",
      "Crawling 362 product: Completed\n",
      "Crawling 363 product: Completed\n",
      "Crawling 364 product: Completed\n",
      "Crawling 365 product: Completed\n",
      "Crawling 366 product: Completed\n",
      "Crawling 367 product: Completed\n",
      "Crawling 368 product: Completed\n",
      "Crawling 369 product: Completed\n",
      "Crawling 370 product: Completed\n",
      "Crawling 371 product: Completed\n",
      "Crawling 372 product: Completed\n",
      "Crawling 373 product: Completed\n",
      "Crawling 374 product: Completed\n",
      "Crawling 375 product: Completed\n",
      "Crawling 376 product: Completed\n",
      "Crawling 377 product: Completed\n",
      "Crawling 378 product: Completed\n",
      "Crawling 379 product: Completed\n",
      "Crawling 380 product: Completed\n",
      "Crawling 381 product: Completed\n",
      "Crawling 382 product: Completed\n",
      "Crawling 383 product: Completed\n",
      "Crawling 384 product: Completed\n",
      "Crawling 385 product: Completed\n",
      "Crawling 386 product: Completed\n",
      "Crawling 387 product: Completed\n",
      "Crawling 388 product: Completed\n",
      "Crawling 389 product: Completed\n",
      "Crawling 390 product: Completed\n",
      "Crawling 391 product: Completed\n",
      "Crawling 392 product: Completed\n",
      "Crawling 393 product: Completed\n",
      "Crawling 394 product: Completed\n",
      "Crawling 395 product: Completed\n",
      "Crawling 396 product: Completed\n",
      "Crawling 397 product: Completed\n",
      "Crawling 398 product: Completed\n",
      "Crawling 399 product: Completed\n",
      "Crawling 400 product: Completed\n",
      "Crawling 401 product: Completed\n",
      "Crawling 402 product: Completed\n",
      "Crawling 403 product: Completed\n",
      "Crawling 404 product: Completed\n",
      "Crawling 405 product: Completed\n",
      "Crawling 406 product: Completed\n",
      "Crawling 407 product: Completed\n",
      "Crawling 408 product: Completed\n",
      "Crawling 409 product: Completed\n",
      "Crawling 410 product: Completed\n",
      "Crawling 411 product: Completed\n",
      "Crawling 412 product: Completed\n",
      "Crawling 413 product: Completed\n",
      "Crawling 414 product: Completed\n",
      "Crawling 415 product: Completed\n",
      "Crawling 416 product: Completed\n",
      "Crawling 417 product: Completed\n",
      "Crawling 418 product: Completed\n",
      "crawled data saved at location W:\\analystt.ai/part2_crawled_data.csv\n",
      "Part 2 crawling completed successfully.....\n"
     ]
    }
   ],
   "source": [
    "crawler = DataCrawler()\n",
    "crawler.main(BASE_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37c566a-999d-49da-aa84-34f5e8ec04fe",
   "metadata": {},
   "source": [
    "<center style = \"color: purple; font-weight: bold\"><i><h2>This is from my side...</h2></i><center>\n",
    "<center style = \"color: purple; font-weight: bold\"><i><h3>Hope this notebook will be informative</h3></i><center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b79cfc6-6845-428e-ac2e-2ca4a3bb03bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
